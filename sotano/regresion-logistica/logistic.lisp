(defpackage :logistic
  (:use :cl))

(in-package :logistic)

(setq *read-default-float-format* 'double-float)

(defun dot (vs ws)
  (apply #'+ (mapcar #'* vs ws)))

;;; dot should compute the dot product between two lists
(assert (equal (dot '(1 2 3) '(4 5 6))
               (+ (* 1 4) (* 2 5) (* 3 6))))

(assert (equal (dot '() '())
               0))

(defun add (vs ws)
  (mapcar #'+ vs ws))

;;; add should compute the point-wise addition for lists
(assert (equal (add '(1 2 3) '(3 2 1))
               '(4 4 4)))

(assert (equal (add '(123) '(321))
               (list (+ 123 321))))

;;; scale should multiply each element of a list by a factor
(defun scale (vs k)
  (mapcar #'(lambda (v) (* v k)) vs))

(assert (equal (scale '(1 2 3) 2)
               (add '(1 2 3) '(1 2 3))))

;;; logistic should compute the logistic function
(defun logistic (s)
  (/ 1.0 (+ 1.0 (exp (- (float s 1.0))))))

(assert (equal (logistic 0) 0.5))

(assert (<= (- 1 (logistic 100))
            0.0000001))

(assert (<= (- (logistic -100) 0)
            0.00000001))

;;; minimize the dataset error using its gradient scaled by a small learning rate
(defun gradient-descent (dataset targets initialize learning-rate limit small-enough-error-p)
  (let ((parameters (funcall initialize))
        (learning-error))
    (dotimes (epoch limit (values parameters learning-error))
      (setq learning-error (dataset-error dataset targets parameters))
      (if (funcall small-enough-error-p learning-error)
          (return (values parameters learning-error))
          (let ((learning-error-gradient (dataset-error-gradient dataset targets parameters)))
            (setq parameters (add parameters (scale learning-error-gradient (- learning-rate)))))))))

;;; compute the error of the whole dataset
(defun dataset-error (dataset targets parameters)
  (loop for features across dataset
     for target across targets
     summing (features-error features target parameters) into total
     finally (return (/ total (length targets)))))

;;; compute the error of a dataset entry
(defun features-error (features target parameters)
  (log (+ 1.0 (exp (- (* target (dot features parameters)))))))

;;; compute the error gradient of the whole dataset
(defun dataset-error-gradient (dataset targets parameters)
  (scale (reduce #'add
                 (map 'vector #'(lambda (features target)
                                  (features-error-gradient features target parameters))
                      dataset targets))
         (/ 1.0 (length targets))))

;;; compute the error gradient of a dataset entry
(defun features-error-gradient (features target parameters)
  (scale features (* (- target) (logistic (* (- target) (dot features parameters))))))

;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
;; classifying Andrew Ng's student tests dataset ;;
;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;

(defvar dataset
  #((1.0 34.62365962451697 78.0246928153624)
    (1.0 30.28671076822607 43.89499752400101)
    (1.0 35.84740876993872 72.90219802708364)
    (1.0 60.18259938620976 86.30855209546826)
    (1.0 79.0327360507101  75.3443764369103)
    (1.0 45.08327747668339 56.3163717815305)
    (1.0 61.10666453684766 96.51142588489624)
    (1.0 75.02474556738889 46.55401354116538)
    (1.0 76.09878670226257 87.42056971926803)
    (1.0 84.43281996120035 43.53339331072109)
    (1.0 95.86155507093572 38.22527805795094)
    (1.0 75.01365838958247 30.60326323428011)
    (1.0 82.30705337399482 76.48196330235604)
    (1.0 69.36458875970939 97.71869196188608)
    (1.0 39.53833914367223 76.03681085115882)
    (1.0 53.9710521485623  89.20735013750205)
    (1.0 69.07014406283025 52.74046973016765)
    (1.0 67.94685547711617 46.67857410673128)
    (1.0 70.66150955499435 92.92713789364831)
    (1.0 76.97878372747498 47.57596364975532)
    (1.0 67.37202754570876 42.83843832029179)
    (1.0 89.67677575072079 65.79936592745237)
    (1.0 50.534788289883   48.85581152764205)
    (1.0 34.21206097786789 44.20952859866288)
    (1.0 77.9240914545704  68.9723599933059)
    (1.0 62.27101367004632 69.95445795447587)
    (1.0 80.1901807509566  44.82162893218353)
    (1.0 93.114388797442   38.80067033713209)
    (1.0 61.83020602312595 50.25610789244621)
    (1.0 38.78580379679423 64.99568095539578)
    (1.0 61.379289447425   72.80788731317097)
    (1.0 85.40451939411645 57.05198397627122)
    (1.0 52.10797973193984 63.12762376881715)
    (1.0 52.04540476831827 69.43286012045222)
    (1.0 40.23689373545111 71.16774802184875)
    (1.0 54.63510555424817 52.21388588061123)
    (1.0 33.91550010906887 98.86943574220611)
    (1.0 64.17698887494485 80.90806058670817)
    (1.0 74.78925295941542 41.57341522824434)
    (1.0 34.1836400264419  75.2377203360134)
    (1.0 83.90239366249155 56.30804621605327)
    (1.0 51.54772026906181 46.85629026349976)
    (1.0 94.44336776917852 65.56892160559052)
    (1.0 82.36875375713919 40.61825515970618)
    (1.0 51.04775177128865 45.82270145776001)
    (1.0 62.22267576120188 52.06099194836679)
    (1.0 77.19303492601364 70.45820000180959)
    (1.0 97.77159928000232 86.7278223300282)
    (1.0 62.07306379667647 96.76882412413983)
    (1.0 91.56497449807442 88.69629254546599)
    (1.0 79.94481794066932 74.16311935043758)
    (1.0 99.2725269292572  60.99903099844988)
    (1.0 90.54671411399852 43.39060180650027)
    (1.0 34.52451385320009 60.39634245837173)
    (1.0 50.2864961189907  49.80453881323059)
    (1.0 49.58667721632031 59.80895099453265)
    (1.0 97.64563396007767 68.86157272420604)
    (1.0 32.57720016809309 95.59854761387875)
    (1.0 74.24869136721598 69.82457122657193)
    (1.0 71.79646205863379 78.45356224515052)
    (1.0 75.3956114656803  85.75993667331619)
    (1.0 35.28611281526193 47.02051394723416)
    (1.0 56.25381749711624 39.26147251058019)
    (1.0 30.05882244669796 49.59297386723685)
    (1.0 44.66826172480893 66.45008614558913)
    (1.0 66.56089447242954 41.09209807936973)
    (1.0 40.45755098375164 97.53518548909936)
    (1.0 49.07256321908844 51.88321182073966)
    (1.0 80.27957401466998 92.11606081344084)
    (1.0 66.74671856944039 60.99139402740988)
    (1.0 32.72283304060323 43.30717306430063)
    (1.0 64.0393204150601  78.03168802018232)
    (1.0 72.34649422579923 96.22759296761404)
    (1.0 60.45788573918959 73.09499809758037)
    (1.0 58.84095621726802 75.85844831279042)
    (1.0 99.82785779692128 72.36925193383885)
    (1.0 47.26426910848174 88.47586499559782)
    (1.0 50.45815980285988 75.80985952982456)
    (1.0 60.45555629271532 42.50840943572217)
    (1.0 82.22666157785568 42.71987853716458)
    (1.0 88.9138964166533  69.80378889835472)
    (1.0 94.83450672430196 45.69430680250754)
    (1.0 67.31925746917527 66.58935317747915)
    (1.0 57.23870631569862 59.51428198012956)
    (1.0 80.36675600171273 90.96014789746954)
    (1.0 68.46852178591112 85.59430710452014)
    (1.0 42.0754545384731  78.84478600148043)
    (1.0 75.47770200533905 90.42453899753964)
    (1.0 78.63542434898018 96.64742716885644)
    (1.0 52.34800398794107 60.76950525602592)
    (1.0 94.09433112516793 77.15910509073893)
    (1.0 90.44855097096364 87.50879176484702)
    (1.0 55.48216114069585 35.57070347228866)
    (1.0 74.49269241843041 84.84513684930135)
    (1.0 89.84580670720979 45.35828361091658)
    (1.0 83.48916274498238 48.38028579728175)
    (1.0 42.2617008099817  87.10385094025457)
    (1.0 99.31500880510394 68.77540947206617)
    (1.0 55.34001756003703 64.9319380069486)
    (1.0 74.77589300092767 89.52981289513276)))

(defvar targets
  #(-1 -1 -1 +1 +1 -1 +1 +1 +1 +1 -1 -1 +1 +1 -1 +1 +1 -1 +1 +1 -1 +1 -1 -1 +1 +1 +1 -1 -1 -1 +1 +1 -1 +1 -1
    -1 -1 +1 -1 -1 +1 -1 +1 -1 -1 -1 +1 +1 +1 +1 +1 +1 +1 -1 -1 -1 +1 -1 +1 +1 +1 -1 -1 -1 -1 -1 +1 -1 +1 +1
    -1 +1 +1 +1 +1 +1 +1 +1 -1 -1 +1 +1 +1 +1 +1 +1 -1 +1 +1 -1 +1 +1 -1 +1 +1 +1 +1 +1 +1 +1))

(defun random-parameter ()
  (/ (random 1000) 1000.0))

(defun initialize ()
  (list (random-parameter)
        (random-parameter)
        (random-parameter)))

(defun less-than (k)
  #'(lambda (x)
      (< x k)))
